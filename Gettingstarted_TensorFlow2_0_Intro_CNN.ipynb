{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gettingstarted-TensorFlow2.0-Intro-CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningTensorFlow2.0/blob/master/Gettingstarted_TensorFlow2_0_Intro_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Bkv9vJHsYg",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Convolutional Neural Networks with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMQbYQxxiG09",
        "colab_type": "text"
      },
      "source": [
        "Let's start by installing tensorflow 2.0. Colab jupyter notebook are shipped with the last stable version of TensorFlow, and since tf 2.0 is still in alpha we need to explicitly install it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTO1h-GFKXMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhZc6WC1ieut",
        "colab_type": "text"
      },
      "source": [
        "After the installation is complete, we import some standard Python libraries, the tensorflow framework and some keras modules. In the end, we check we're using the right tensorflow version, which should be 2.0.0-alpha0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3mGUhZwKgnw",
        "colab_type": "code",
        "outputId": "a1016c07-bf44-46cb-ee26-959caf905a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#import print function from future\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "#import TensorFlow and check version\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVXbCSRg37JC",
        "colab_type": "text"
      },
      "source": [
        "**MNIST Dataset**\n",
        "\n",
        "For this introduction to convolutional neural networks with Tensorflow 2.0, we'll be using the MNIST dataset. First we download the dataset and then normalize it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfVB3Nbs4Qpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ad327c85-cdd7-41df-939e-f3350600f2e7"
      },
      "source": [
        "mnist = datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foW5QWBLjiv0",
        "colab_type": "code",
        "outputId": "f13e1afa-80ab-44c2-eece-7ebe08136b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xJrZRrHlZb2",
        "colab_type": "code",
        "outputId": "1dbb2411-2f94-4ac3-c112-53f36084f00a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwmPRnQXhS0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_HuGtEOkjhN",
        "colab_type": "code",
        "outputId": "30b3525b-941a-456e-c2a8-d7a55217c75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enurh7lRmTE2",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Bookkeeping: Parameters\n",
        "\n",
        "Now let's find out how the TensorFlow calculated the parameters for each convolutional layer. \n",
        "\n",
        "***1st convolutional layer***\n",
        "\n",
        "In the first convolutional layer we have 32 kernels or filters of 3x3. \n",
        "Each filter has 9 (3x3) parameters and that gives us a total of 288 parameters: 32 x 9.\n",
        "But at the same time, each one of these filters has a bias: 32 x 1\n",
        "\n",
        "32 x 9 weights + 32 x 1 biases = 320 parameters\n",
        "\n",
        "***2nd convolutional layer***\n",
        "\n",
        "After applying the first pooling we end up with 32 feature maps, and that's the input for our second convolutional layer. In the second layer we use 64 filters of 3x3, but now we apply these 64 filter to each of the 32 feature maps.\n",
        "\n",
        "Each filter has 9 (3x3) parameters and that gives us a total of 576 parameters: 64 x 9. And each one of the filters has a bias: 64  x 1\n",
        "\n",
        "32 x (64 x 9 weights) + 64 x 1 biases = 18496 parameters.\n",
        "\n",
        "***3rd convolutional layer***\n",
        "\n",
        "After applying the second polling we end up with 64 feature maps. In the third layer we also use 64 filters of 3x3, and we apply these filters to the 64 feature maps.\n",
        "\n",
        "Each filter has 9 (3x3) parameters and that gives us a total of 576 parameters: 64 x 9. And each one of the filters has a bias: 64  x 1\n",
        "\n",
        "64 x (64 x 9 weights) + 64 x 1 biases = 36928 parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMN15EK-Y5W",
        "colab_type": "text"
      },
      "source": [
        "### Dense Classifier\n",
        "\n",
        "We now add a dense layer and final layer with 10 neurons, one for each of the digit classes (one, two, three, etc). Because of the way in which dense layers work we need to convert the 3D tensor into a vector, and for that task we use a Flatten layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXta0j02_n38",
        "colab_type": "code",
        "outputId": "99b78e19-8155-4891-f6fd-64540dc46fbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Let's check our neural network architecture again\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kqz8lHHzZ42",
        "colab_type": "text"
      },
      "source": [
        "Now let's do the math for the rest of the neural network. First we have to stretch out the output tensor of our 3rd convolution layer:\n",
        "\n",
        "tensor dimensions = (3, 3, 64) --> 3 x 3 x 64 = 576 elements vector\n",
        "\n",
        "That's the input layer dimension of the second part of our neural network. So now we have a vector of length 576, a hidden dense layer composed of 64 neurons and an output layer of 10 neurons.\n",
        "\n",
        "Each of the dense layer's neurons are connected to all the neurons in the input layer. And each of the 64 neurones has a bias:\n",
        "\n",
        "64 x 576 weights + 64 x 1 bias = 36928 parameters\n",
        "\n",
        "And finally, for our output layer we have:\n",
        "\n",
        "10 * 64 weights + 10 biases = 640 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-CTazngQ20E",
        "colab_type": "text"
      },
      "source": [
        "### Neurons and Tensor Dimensions\n",
        "\n",
        "Now let's a look at each layers' neurons and how the input tensor is transformed as it goes through the neural network.\n",
        "\n",
        "Our MNIST dataset contains grayscale images of 28x28, this means that it contains only one channel. Therefore input tensor dimensions are:\n",
        "\n",
        "(28, 28, 1) = 576 neurons\n",
        "\n",
        "In the case of the convolution layers, the neurons live in the feature maps. For the first convolutional layer we have 32 feature maps (given by the numbers of filters), and we use a formula to calculate the width and height for a particular feature map:\n",
        "\n",
        "w = (w - f + 2p) / s + 1 --> width: (28 - 3 + 2x0) / 1 + 1 = 26\n",
        "\n",
        "h = (h - f + 2p) / s + 1 --> height: (28 - 3 + 2x0) / 1 + 1 = 26\n",
        "\n",
        "f is the size of the filter; p is the padding; s is the stride.\n",
        "\n",
        "(26, 26, 32) = 21632 neurons. That's a lot, but after applying max pooling, which outputs the maximum activation given a max pooling of 2 x 2, we end up with:\n",
        "\n",
        "(13, 13, 32) = 5408 neurons.\n",
        "\n",
        "This convolution process continues until we reach the last convolution layer and we end up with:\n",
        "\n",
        "(3, 3, 64) = 576 neurons and we are ready to add our dense layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ1SZzgXmMXB",
        "colab_type": "text"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fJH52grmXNh",
        "colab_type": "code",
        "outputId": "78c9b50c-a154-45b4-e0fd-a93e1512d6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 45s 756us/sample - loss: 0.1415 - accuracy: 0.9566\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 46s 774us/sample - loss: 0.0447 - accuracy: 0.9862\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 44s 731us/sample - loss: 0.0309 - accuracy: 0.9901\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 45s 755us/sample - loss: 0.0246 - accuracy: 0.9918\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 43s 709us/sample - loss: 0.0186 - accuracy: 0.9940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2e2c062ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WaVrNtn9LC7",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating accuracy\n",
        "\n",
        "So far we have measured the accuracy for our training set, so now we determine the accuracy for our test set (the unseen data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCv_tyjb9mAj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0eaa7513-4f1a-499b-f5f8-2d5a45f3e60e"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 275us/sample - loss: 0.0448 - accuracy: 0.9861\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}