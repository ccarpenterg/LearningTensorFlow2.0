{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gettingstarted_TensorFlow2_0_Pre-trained_Models.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningTensorFlow2.0/blob/master/05_pretrained_convnets_and_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdy7rZpLB8DB",
        "colab_type": "text"
      },
      "source": [
        "# Pretrained Convolutional Networks\n",
        "\n",
        "Before trying to tackle the problem of overfitting, we will explore the concept of a pre-trained convnet. So this is where the world of AI gets really exciting. We not only can use a great ML framework like TensorFlow, developed by Google, one the most advanced companies in terms of AI, but we also can download a pretrained convolutional neural network, that has been trained by a company like Google or by a research institution like Stanford.\n",
        "\n",
        "\n",
        "That means that years and years of research are **available** to everybody, provided they have the technical skillls to use these pretrained convolutional neural networks.\n",
        "\n",
        "Let's start by installing tensorflow 2.0 and checking we have the right version:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WchLDAhE97QO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9F5xKnx6Q9F",
        "colab_type": "code",
        "outputId": "e64e6983-68d7-4964-e581-59db83f129df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPn7ehNhwHcO",
        "colab_type": "text"
      },
      "source": [
        "## VGG16\n",
        "\n",
        "We'll start by exploring the VGG16 network. It was developed by the Visual Geometry Group at Oxford University, hence the name VGG. It has 16 layers, including convolutional and fullly connected layers, and a little more than 138 million parameters (including weights and biases).\n",
        "\n",
        "We can download the convnet structure and parameters through the Keras module by instantiating the class VGG16:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYxCd1zEpvBy",
        "colab_type": "code",
        "outputId": "3e8bf8ce-aec0-4c0c-c3ab-d20fc171816c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "convnet = VGG16()\n",
        "\n",
        "convnet.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBrDNJ3RZKbf",
        "colab_type": "text"
      },
      "source": [
        "So this is pretty much like downloading a brain, or a section a of a brain to be more precise. Now keep in mind that we have to re-train parts of a pretrained network so that we can apply the knowledge of this network to our particular problem.\n",
        "\n",
        "Most of the time the pretrained convnet has been trained on large datatasets such as Imagenet and with very different classes, so in order to apply it to our problem we will have to remove the dense classifier, which are the fully connected layers that sit on top of the convolutional and polling layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJW7lm5nLq-s",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction and Fine-tuning\n",
        "\n",
        "So why do we want to use a pre-trained convnet if its cassifier is not able to classify our images? The answer is **Feature extraction**. A convnet's architecture consists of two parts: the convolutional base (including convolutional and pooling layers), and the classifier (fully connected layers).\n",
        "\n",
        "The convolutional base automatically extract the features that then are fed to the fully conntected layers for classification. Since the features extracted by the convolutional base are universal, in the sense that these are visual features that are part of every object (lines, edges, tones, etc), we can re-use this convolutional base to extract features from our dataset.\n",
        "\n",
        "If we use the analogy of the human brain, we can say that the visual cortex is equivalent to our convolutional layers. They extract features like horizontal lines, vertical lines, curves, edges, etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygXj3QmTDmbL",
        "colab_type": "code",
        "outputId": "7b67bc2d-493e-4727-f40b-b8b4158a7810",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(224, 224, 3))\n",
        "\n",
        "conv_base.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8js8MfTTmf1x",
        "colab_type": "text"
      },
      "source": [
        "### Convolution and Pooling Operations\n",
        "\n",
        "Now we take a look at the convolutional and pooling operations, and how they were setup for the VGG16 network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCcY-xj6ep9n",
        "colab_type": "code",
        "outputId": "fc78174b-2a44-4c4c-c5a2-7863f77636b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "for layer in conv_base.layers:\n",
        "    if 'conv' in layer.name:\n",
        "        print('{} {} kernels of {}, stride of {} and {} padding'.format(\n",
        "                                                                    layer.name,\n",
        "                                                                    layer.filters,\n",
        "                                                                    layer.kernel_size,\n",
        "                                                                    layer.strides,\n",
        "                                                                    layer.padding))\n",
        "        \n",
        "    if 'pool' in layer.name:\n",
        "        print('{} of size {}, stride of {} and {} padding'.format(\n",
        "                                                                layer.name,\n",
        "                                                                layer.pool_size,\n",
        "                                                                layer.strides,\n",
        "                                                                layer.padding))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "block1_conv1 64 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block1_conv2 64 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block1_pool of size (2, 2), stride of (2, 2) and valid padding\n",
            "block2_conv1 128 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block2_conv2 128 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block2_pool of size (2, 2), stride of (2, 2) and valid padding\n",
            "block3_conv1 256 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block3_conv2 256 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block3_conv3 256 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block3_pool of size (2, 2), stride of (2, 2) and valid padding\n",
            "block4_conv1 512 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block4_conv2 512 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block4_conv3 512 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block4_pool of size (2, 2), stride of (2, 2) and valid padding\n",
            "block5_conv1 512 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block5_conv2 512 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block5_conv3 512 kernels of (3, 3), stride of (1, 1) and same padding\n",
            "block5_pool of size (2, 2), stride of (2, 2) and valid padding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KrwtO2vruWB",
        "colab_type": "text"
      },
      "source": [
        "**Convolution**\n",
        "\n",
        "The base convolution uses a 3x3 filter (kernel) with a stride of 1 and same padding. As you can see, depending on the depth of the layer it applies 64, 128, 256 or 512 filters. The same padding means that it uses padding in such a way that input and output height and width are the same.\n",
        "\n",
        "**Max Pooling**\n",
        "\n",
        "VGG16 uses max pooling of 2x2 with a stride of 2 for the pooling layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUqK4RL3U1qC",
        "colab_type": "text"
      },
      "source": [
        "### Freezing Layers and Preparing for Fine-Tuning\n",
        "\n",
        "In order to fine-tune our convolutional neural network we need to freeze the convolutional base. Since we're are using tensorflow's Keras API this is really straightforward; we just set the conv_base's parameter trainable to False:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtjh8bIbIVm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "f941c8ae-991c-4bd3-d83a-d4d1c138b135"
      },
      "source": [
        "conv_base.trainable = False\n",
        "conv_base.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRjbiiaCMTfG",
        "colab_type": "text"
      },
      "source": [
        "Now we see that there are zero trainable parameters, which means that VGG16's convolutional base has been frozen and we're ready to add a new classifier to our convolutional neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0_QlG86kmQe",
        "colab_type": "text"
      },
      "source": [
        "### Other Pretrained Convnets\n",
        "\n",
        "\n",
        "\n",
        "*   Xception\n",
        "*   ResNet\n",
        "*   Inception\n",
        "*   MobileNet\n",
        "*   DenseNet\n",
        "*   NASNet\n",
        "\n"
      ]
    }
  ]
}