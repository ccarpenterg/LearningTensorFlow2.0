{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gettingstarted-TensorFlow2.0-Intro-CNN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningTensorFlow2.0/blob/master/02_introduction_to_convnets_and_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Bkv9vJHsYg",
        "colab_type": "text"
      },
      "source": [
        "### Introduction to Convolutional Neural Networks with TensorFlow 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMQbYQxxiG09",
        "colab_type": "text"
      },
      "source": [
        "Convolutional neural networks (CNNs or convnets for short) are one of most exciting new developments in the field of computer vision. A key feature of CNNs is that feature extraction is carried out by the convolutional layers, and there is no need for feature engineering.\n",
        "\n",
        "### GPUs in Colab\n",
        "We'll be using the GPU that is provided by Google in Colab, so in order to enable the GPU for this notebook, follow the next steps:\n",
        "\n",
        "* Navigate to **Edit** â†’ **Notebook settings**\n",
        "* Open the **Hard accelerator** drop-down menu and select **GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhZc6WC1ieut",
        "colab_type": "text"
      },
      "source": [
        "# Image classification using a Convnet with Tensorflow 2\n",
        "\n",
        "we import some standard Python libraries, the tensorflow framework and some keras modules. In the end, we check we're using the right tensorflow version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3mGUhZwKgnw",
        "colab_type": "code",
        "outputId": "522111f9-d988-412f-f770-122b8d87f18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#import print function from future\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "#import TensorFlow and check version\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVXbCSRg37JC",
        "colab_type": "text"
      },
      "source": [
        "**MNIST Dataset**\n",
        "\n",
        "For this introduction to convolutional neural networks with Tensorflow 2.0, we'll be using the MNIST dataset. First we download the dataset and then normalize it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfVB3Nbs4Qpm",
        "colab_type": "code",
        "outputId": "8dbaf193-7e91-46ae-e9dd-2f06d9d41bcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "mnist = datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqg5EnCqC6sN",
        "colab_type": "text"
      },
      "source": [
        "Since hhe MNIST images have only one channel, we need to explicitly reshape them to include a fourth dimension (that one channel):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xJrZRrHlZb2",
        "colab_type": "code",
        "outputId": "eac5bd29-e94b-4cd1-f36e-8ca5955465ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "x_train = x_train.reshape((60000, 28, 28, 1))\n",
        "x_test = x_test.reshape((10000, 28, 28, 1))\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BpXzl8jK8lZ",
        "colab_type": "text"
      },
      "source": [
        "We begin with 3 convolutional layers. The first one has 32 filters of 3x3 pixels, takes our MNIST images as input (28, 28, 1) and applies RELU as the activation function. The output of this operations are 32 feature maps, over which we then apply max-pooling of 2x2 pixels. The same operations are repeated in a similar way for the other convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwmPRnQXhS0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGmy_N3nlWFK",
        "colab_type": "text"
      },
      "source": [
        "Now we can get a summary of this convolutional neural network. Our model includes the method summary, which will show us the tensor shape for each layer and how many parameters will be calculated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_HuGtEOkjhN",
        "colab_type": "code",
        "outputId": "6bf757cd-404f-4934-94a9-d5b33f690aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "=================================================================\n",
            "Total params: 55,744\n",
            "Trainable params: 55,744\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enurh7lRmTE2",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Bookkeeping: Parameters\n",
        "\n",
        "Now let's find out how the TensorFlow calculated the parameters for each convolutional layer. \n",
        "\n",
        "***1st convolutional layer***\n",
        "\n",
        "In the first convolutional layer we have 32 kernels or filters of 3x3. \n",
        "Each filter has 9 (3x3) parameters and that gives us a total of 288 parameters: 32 x 9.\n",
        "But at the same time, each one of these filters has a bias: 32 x 1\n",
        "\n",
        "32 x 9 weights + 32 x 1 biases = 320 parameters\n",
        "\n",
        "***2nd convolutional layer***\n",
        "\n",
        "After applying the first pooling we end up with 32 feature maps, and that's the input for our second convolutional layer. In the second layer we use 64 filters of 3x3, but now we apply these 64 filter to each of the 32 feature maps.\n",
        "\n",
        "Each filter has 9 (3x3) parameters and that gives us a total of 576 parameters: 64 x 9. And each one of the filters has a bias: 64  x 1\n",
        "\n",
        "32 x (64 x 9 weights) + 64 x 1 biases = 18496 parameters.\n",
        "\n",
        "***3rd convolutional layer***\n",
        "\n",
        "After applying the second polling we end up with 64 feature maps. In the third layer we also use 64 filters of 3x3, and we apply these filters to the 64 feature maps.\n",
        "\n",
        "Each filter has 9 (3x3) parameters and that gives us a total of 576 parameters: 64 x 9. And each one of the filters has a bias: 64  x 1\n",
        "\n",
        "64 x (64 x 9 weights) + 64 x 1 biases = 36928 parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keMN15EK-Y5W",
        "colab_type": "text"
      },
      "source": [
        "### Dense Classifier\n",
        "\n",
        "We now add a dense layer and final layer with 10 neurons, one for each of the digit classes (one, two, three, etc). Because of the way in which dense layers work we need to convert the 3D tensor into a vector, and for that task we use a Flatten layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXta0j02_n38",
        "colab_type": "code",
        "outputId": "83a82be6-6730-4287-89b3-cc53395004a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Let's check our neural network architecture again\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kqz8lHHzZ42",
        "colab_type": "text"
      },
      "source": [
        "Now let's do the math for the rest of the neural network. First we have to stretch out the output tensor of our 3rd convolution layer:\n",
        "\n",
        "tensor dimensions = (3, 3, 64) --> 3 x 3 x 64 = 576 elements vector\n",
        "\n",
        "That's the input layer dimension of the second part of our neural network. So now we have a vector of length 576, a hidden dense layer composed of 64 neurons and an output layer of 10 neurons.\n",
        "\n",
        "Each of the dense layer's neurons are connected to all the neurons in the input layer. And each of the 64 neurones has a bias:\n",
        "\n",
        "64 x 576 weights + 64 x 1 bias = 36928 parameters\n",
        "\n",
        "And finally, for our output layer we have:\n",
        "\n",
        "10 * 64 weights + 10 biases = 650 parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-CTazngQ20E",
        "colab_type": "text"
      },
      "source": [
        "### Neurons and Tensor Dimensions\n",
        "\n",
        "Now let's a look at each layers' neurons and how the input tensor is transformed as it goes through the neural network.\n",
        "\n",
        "Our MNIST dataset contains grayscale images of 28x28, this means that it contains only one channel. Therefore input tensor dimensions are:\n",
        "\n",
        "(28, 28, 1) = 784 input neurons\n",
        "\n",
        "In the case of the convolution layers, the neurons live in the feature maps. For the first convolutional layer we have 32 feature maps (given by the numbers of filters), and we use a formula to calculate the width and height for a particular feature map:\n",
        "\n",
        "w = (w - f + 2p) / s + 1 --> width: (28 - 3 + 2x0) / 1 + 1 = 26\n",
        "\n",
        "h = (h - f + 2p) / s + 1 --> height: (28 - 3 + 2x0) / 1 + 1 = 26\n",
        "\n",
        "f is the size of the filter; p is the padding; s is the stride.\n",
        "\n",
        "(26, 26, 32) = 21632 neurons. That's a lot, but after applying max pooling, which outputs the maximum activation given a max pooling of 2 x 2, we end up with:\n",
        "\n",
        "(13, 13, 32) = 5408 neurons.\n",
        "\n",
        "This convolution process continues until we reach the last convolution layer and we end up with:\n",
        "\n",
        "(3, 3, 64) = 576 neurons and we are ready to add our dense layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ1SZzgXmMXB",
        "colab_type": "text"
      },
      "source": [
        "### Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fJH52grmXNh",
        "colab_type": "code",
        "outputId": "802a9793-1335-4a92-9c4c-cac5306cf809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 12s 194us/sample - loss: 0.1555 - accuracy: 0.9517\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0486 - accuracy: 0.9847\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0347 - accuracy: 0.9891\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0263 - accuracy: 0.9914\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0202 - accuracy: 0.9935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd060462e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WaVrNtn9LC7",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating accuracy\n",
        "\n",
        "So far we have measured the accuracy for our training set, so now we determine the accuracy for our test set (the unseen data):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCv_tyjb9mAj",
        "colab_type": "code",
        "outputId": "407645d3-f79d-47b0-d508-15f56271151c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 83us/sample - loss: 0.0395 - accuracy: 0.9874\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}